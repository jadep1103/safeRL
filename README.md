# ğŸš€ SafeRL: Modernized Safe Reinforcement Learning Implementation

A modern reimplementation of the **Trust Region Conditional Value at Risk (TRC)** algorithm using updated libraries and compared against **Proximal Policy Optimization (PPO)** as a baseline. This project updates the original implementation from older MuJoCo versions to modern mujocopy &`safety-gymnasium` environments.

> **Based on:** *TRC: Trust Region Conditional Value at Risk for Safe Reinforcement Learning*  
> Dohyeong Kim and Songhwai Oh, *IEEE Robotics and Automation Letters*, 2022.

## ğŸ¯ What is SafeRL?

This project demonstrates **safe reinforcement learning** - teaching AI agents to achieve goals while respecting safety constraints. Unlike standard RL that only maximizes rewards, safe RL also minimizes constraint violations (like collisions or unsafe behaviors).

### ğŸ”¬ Algorithms Compared

| Algorithm | Type | Key Features |
|-----------|------|-------------|
| **PPO** | Standard RL | Fast, stable, but ignores safety constraints |
| **TRC** | Safe RL | Uses Conditional Value at Risk (CVaR) to ensure safety |

### ğŸŸï¸ Environments

The agents are tested on **Safety-Gymnasium** environments:
- **SafetyPointGoal1/2-v0**: Point robot navigation with obstacles
- **SafetyCarGoal1/2-v0**: Car navigation avoiding hazards  
- **SafetyDoggoGoal1-v0**: Quadruped robot locomotion

### ğŸ”„ Sim-to-Real Transfer Experiments

A key experimental focus was exploring **sim-to-real transfer** in safe RL by treating different Safety-Gymnasium difficulty levels as "simulation" vs "real" environments:

- **"Simulation"**: Goal1 environments (easier safety constraints)
- **"Real World"**: Goal2 environments (harder safety constraints, more obstacles)

This setup allowed us to study how well safety policies trained in easier conditions transfer to more challenging scenarios - a critical question for deploying safe RL in real-world applications.

---

## ğŸ› ï¸ Quick Setup

### Prerequisites
- Python 3.10+ (recommended for compatibility)
- GPU optional but recommended for training

### Option 1: Local Installation
```bash
# Clone the repository
git clone <your-repo-url>
cd safeRL-1

# Create virtual environment
python -m venv saferl-env
source saferl-env/bin/activate  # or `saferl-env\Scripts\activate` on Windows

# Install dependencies
pip install -r requirements.txt
```

### Option 2: Docker (may be tricky for env render)
```bash
docker build -t safe-rl .
docker run --rm -it safe-rl
```

---

## ğŸš€ Usage

### Training Agents

#### PPO (Baseline)
```bash
# Train PPO on car environment
python ppo/train.py --env SafetyCarGoal1-v0 --timesteps 1000000 --num_envs 4

# Available environments
python ppo/train.py --env SafetyPointGoal1-v0
python ppo/train.py --env SafetyDoggoGoal1-v0
```

#### TRC (Safe RL)
```bash
# Train TRC with safety constraints
python trc/main.py --env_name SafetyPointGoal1-v0 --n_envs 1 --name TRC_Point --wandb

# Training with multiple environments (recommended)
python trc/main_multi.py --env_name SafetyCarGoal1-v0 --n_envs 4 --name TRC_Car_Multi --wandb
```

### Testing Trained Models

#### PPO Testing
```bash
python ppo/test.py --env SafetyCarGoal1-v0 --episodes 10
```

#### TRC Testing  
```bash
python trc/main.py --env_name SafetyPointGoal1-v0 --test --name TRC_Point_Test --wandb
```

### Sim-to-Real Transfer Testing
```bash
# Train on "simulation" (Goal1 - easier)
python trc/main.py --env_name SafetyPointGoal1-v0 --name TRC_Point_Sim --wandb

# Test on "real world" (Goal2 - harder) 
python trc/main.py --env_name SafetyPointGoal2-v0 --test --name TRC_Point_Transfer --wandb

# Compare performance and safety violations between domains
```

### Batch Training/Testing
```bash
# Train PPO on all environments
python ppo/multi_train_launcher.py

# Test all environments
python ppo/multi_test_launcher.py
```

---

## ğŸ“Š Monitoring & Visualization

### Weights & Biases Integration
Enable W&B logging with `--wandb` flag to track:
- **Reward/Score**: Task performance
- **Cost/Constraint Violations**: Safety metrics  
- **CVaR**: Conditional Value at Risk for tail risk
- **Training Loss**: Policy and value function losses

---

## ğŸ—ï¸ Project Structure

```
safeRL-1/
â”œâ”€â”€ ğŸ“ ppo/                          # PPO implementation (baseline)
â”‚   â”œâ”€â”€ train.py                     # Main training script
â”‚   â”œâ”€â”€ test.py                      # Testing trained models
â”‚   â”œâ”€â”€ wrapper.py                   # Safety-Gymnasium compatibility
â”‚   â”œâ”€â”€ multi_train_launcher.py      # Batch training
â”‚   â”œâ”€â”€ multi_test_launcher.py       # Batch testing  
â”‚   â””â”€â”€ models/                      # Saved PPO models
â”‚       â”œâ”€â”€ SafetyCarGoal1-v0/
â”‚       â”œâ”€â”€ SafetyPointGoal1-v0/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“ trc/                          # TRC implementation (safe RL)
â”‚   â”œâ”€â”€ main.py                      # Training/testing (single env)
â”‚   â”œâ”€â”€ main_multi.py                # Multi-environment training
â”‚   â”œâ”€â”€ agent.py                     # TRC agent implementation
â”‚   â”œâ”€â”€ models.py                    # Neural network architectures
â”‚   â”œâ”€â”€ logger.py                    # Training metrics logging
â”‚   â”œâ”€â”€ utils/                       # Utility scripts
â”‚   â”œâ”€â”€ bash_files/                  # Training scripts
â”‚   â””â”€â”€ results/                     # TRC training results
â”‚       â”œâ”€â”€ TRC_FINAL_car_s42/
â”‚       â”œâ”€â”€ TRC_FINAL_point_s42/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ Dockerfile                       # Container setup
â””â”€â”€ README.md                        # This file
```

---

## âš™ï¸ Key Parameters

### TRC Algorithm
- `--cost_d`: Safety constraint threshold (default: 0.025)
- `--cost_alpha`: CVaR confidence level (default: 0.125)  
- `--max_kl`: Trust region constraint (default: 0.001)
- `--n_epochs`: Policy update epochs (default: 200)

### Training
- `--total_steps`: Total training steps (default: 10M)
- `--n_envs`: Parallel environments (default: 1 for TRC, 4 for PPO)
- `--seed`: Random seed for reproducibility

---

## ğŸ”¬ Results & Analysis

The project includes pre-trained models and results in:
- `ppo/models/`: Trained PPO agents
- `trc/results/`: TRC training logs and checkpoints

### Key Metrics to Compare:
1. **Episode Return**: Total reward achieved
2. **Constraint Violations**: Safety performance  
3. **Training Stability**: Convergence behavior
4. **Sample Efficiency**: Steps needed to learn
5. **Transfer Performance**: How well Goal1â†’Goal2 policies transfer
6. **Safety Degradation**: Constraint violation increase during transfer

---

## ğŸ’¡ Fun Ideas for Extension

Project was mostly academic but could be extended for:

1. **Domain Adaptation**: Extend sim-to-real transfer to actual physical robots ?
3. **Custom Environments**: Create your own safety-critical scenarios
4. **Hyperparameter Tuning**: Optimize safety vs. performance trade-offs
5. **Visualization**: Create cool demos of safe vs. unsafe behaviors
6. **Transfer Learning**: Study how different environment pairs affect transfer success


---

## ğŸ› Known Issues

- TRC currently works best with `n_envs=1` (single environment)
- GPU memory requirements can be high for large batch sizes

---

## ğŸ“š References

- [TRC Paper](https://ieeexplore.ieee.org/document/9830207) - Original TRC algorithm
- [Safety-Gymnasium](https://safety-gymnasium.readthedocs.io/) - Modern safety RL environments  
- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/) - PPO implementation

---

## ğŸ‘¥ Contributors

- **Jade Piller-Cammal**
- **Estelle Tournassat** 

*Originally developed for IFT-7201 (Reinforcement Learning) at UniversitÃ© Laval*

---

## ğŸ“„ License

MIT License
